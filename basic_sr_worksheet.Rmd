---
title: "basic_sr_demo"
author: "Papoula Petri-Romão"
date: "2024-08-29"
output: 
  html_document:
    toc: true
    toc_float:
      collapsed: true
    toc_depth: 3
    number_sections: false
    theme: lumen
---

This is the worksheet for the basic SR score demonstration. 

You will find solutions in the document entitled basic_sr_demo. 

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE)

# This is the "setup" chunk. It is best practice to specify all the packages you will use here.

# the pacman package allows you to manage packages easily, since it will install any packages you don't have and simply load those packages you do have
require(pacman)

# now load packages
p_load(
  tidyverse, # for data management
  lme4, # for multilevel modelling
  ggcorrplot, # for plotting correlations
  ggsignif
)
```


We are using the publically available [Dynacore-L dataset available on OSF](https://osf.io/d6wgr/). The main analyses of this dataset were published by [Boegemann et al. in 2022](https://mental.jmir.org/2023/1/e46518). The calculations here will slightly differ from these main analyses for the purpose of exemplifying SR calculation. 

We have processed the data set to be a bit easier to work with for the purpose of this workshop, but the data remains the same. 

You will find the ressymp_workshop data on our github. 

```{r loaddata}

ds <- read_csv("ressymp_workshop.csv")

```

# Basic SR score

The "Stressor Reactivity" is the reaction as expressed in mental health problems (P) to stressors (E). Therefore you need to identify the stressor exposure measures (E) and measures of mental health problems/symptoms in your dataset (P). 

## 1 Identify your E and P 

In Dynacore-L we have two types of stressors, Corona-related stressors and General stressors. 

* CE : Corona-related stressors, 29 items; 0-5, 0= did not happen. 
* GE : General stressors, 11 items; 0-5, 0= did not happen. 

CE_30 and GE_12 are open questions and are not used for this scoring. 

      Task 1a: Find stressor items in the dataset and look at this subset of variables. Check that the range of the items is as expected. 
      


As P we have the GHQ (CM in the dataset). 

CM : 0-3, where 0 is did not happen.

      Task 1b: Find mental health items in the dataset and look at this subset of variables. Check that the range of the items is expected. 
      



### 1.1 building sum scores

#### 1.1.1 P

Score your P according to the manual. 
For GHQ this is a sum score of all items. 

      Task 1.1.1a: Calculate the sum score of GHQ (P)
      



#### 1.1.2 E

Dependent on your data set you might have different considerations on how you will build your E sum score.

If you have different type of stressors within the same data set, for example, life events and daily hassles, or general and pandemic stressors, you will in most cases want to built separate sum scores for each type of stressor first. 

      Task 1.1.2a: Create two sum scores, one for the general stressors (Eg), one for the corona-related stressors (Es). 
      



#### 1.1.3 to scale or not to scale?

Now the question is how could one combine the E scores. Usually, you want to built the SR score based on the most information available, meaning all types of available stressors. There are two options: 

1. you could sum all items. This works when all stressors are measured or coded the same way and on the same scale(for example all of them indicate occurrence). This would mean that if the type of stressors are not equally represented, the one type for which there are more items will be weighted more. In our example, there are more CE (29 items) then general stressors (11 items). 
2. you could create a mean of the scaled sum scores. this would allow both type of stressors to be equally weighted. This is the approach we have used in MARP and LORA which is described in the [FRESHMO paper](https://www.frontiersin.org/articles/10.3389/fpsyg.2021.710493). 

      Task 1.1.3: create a scaled combined E score (E_scaled) 




      
#### 1.1.4 per timepoint/over time periods

SR is expected to be quite stable and change over longer periods of times. That is a sudden or short change in SR is usually not indicative of less resilient outcomes, rather it is the SR over longer time periods that is more informative on the resilience of participants. 
Consequently, in longitudinal studies we are often interested in longer time periods rather than single time points. For example in the longitudinal samples MARP and LORA we look at period of 9 months to ca. 3.5 years [see Petri-Romão et al.](https://osf.io/dgx4k). 
For this purpose we create the mean stressor exposure and mean P in the time period of interest. 

      Task 1.1.4a: Create mean E scores (Eg_t1.t6, Es_t1.t6) across all timepoints
      



      Task 1.1.4b: Create mean P scores (P_t1.t6) across all timepoints
      


In the case of the combined E score the order of operations is as follows

1. create mean sum scores for each time period (separately for Eg, Es)
2. scale each separate sum score. This means we are scaling over the time period of interest
3. create combined mean score of the scaled scores

      Task 1.1.4c: Create combined E scores over the whole time period E_t1.t6, E_t1.t6_scaled


### 1.2 first look into E and P

First we check that our scores are reasonable. Plotting and summarising the scores makes sense to check that you have done everything correctly and give you a sense if there are outliers. We will not look into the treatment of outliers now, but as good practice you should get used to plotting and checking your data.

      Task 1.2a: Describe the scores
      Generate descriptive statistics of the scores you build to check their quality. (e.g. mean, median, min, max)
      EXTRA: plot the scores. (e.g. scatter, histogram, boxplot...)
      
      Task 1.2b: Interpret your results
      * did you do everything correctly? or are there strange scores, eg. outside of the possible range?
      * do you think there are outliers?





### 1.3 Choosing E 

We want to choose an E score based on a robust (that is high and interpretable) relationship with P. 

      Task 1.3a: Calculate the correlations between E an P 
      Calculate them between all scores or just one, depending on your knowledge of coding.
      EXTRA: plot the correlation matrix for easy interpretation.

      Task 1.3b: Interpret the scores
      * which E score has the highest correlation with P?
      * what do the separate E scores' correlations tell you about their influence on P? (is Eg or Es more important)
      * does the scaling of different E types before summation have an impact on the relationship?
      * what could be a reason to choose separate scores rather than combined scores?
      * which E score would you use?







## 2 Model for normative stressor reactivity

The basis of the SR score for each individual is the presumed normative relationship between E and P, that is the normative reactivity in terms of mental health problems to the exposure of stressors. 
In this part we will calculate the normative E~P line. 

Usual considerations

* use all available data in completed studies
* pre-define acceptable amount of incomplete assessments (e.g. participants must have completed 2/3 assessments)
* you will usually need multiple time points

### 2.1 multilevel vs simple modelling

In cross-sectional samples you will always have to use simple modelling. 
However, in longitudinal samples you can either use multilevel modelling or choose to average over all time points to use simple modelling. The resulting SR scores usually are highly correlated and it is mostly a conceptual distinction. 

In studies were data collection is ongoing studies we have so far used the second appraoch, that is averaging over time points and building simple models. See the FRESHMO paper [Kalisch et al.](https://www.frontiersin.org/articles/10.3389/fpsyg.2021.710493)
For further details refer to the advanced sr script, where the use of a "reference time period" is discussed. 

In studies where you compare different time points, for example intervention studies, we have chosen multilevel modelling (see [Petri-Romão et al. (2024)](https://osf.io/dgx4k)). In completed studies we have also used multilevel modelling, see [Boegemann et al. in 2022](https://mental.jmir.org/2023/1/e46518)

#### 2.1.1 simple modelling

In a cross-sectional study or if computing the normative EP line in a single timepoint you will always use a simple model. 

For this you will define your P of interest (your predicted variable y) and your E in the corresponding time period (your x). 

For the sake of this example we will look at the average mental health problems over all assessments (P_t1.t6) and average stressor exposure in that time period (E_t1.t6). 

      Task 2.1.1a Build a simple linear model predicting P with E. Save the summary output in an object. 





#### 2.1.2 multilevel modelling

Multilevel modelling allows us to build models with repeated measures of E and P. 
It has been used in Dynacore-L and the RESPOND-RCT intervention study. 
It is particularly useful when the study is completed and if averaging would obscure the effect of interest. 

Note that in this example the mixed model results in an (almost / near) singular fit, probably because the inclusion of random slopes overfits the data. However, for SR calculation, we do not interpret the overall model fit and only use the fixed effects.

      Task 2.1.2 Build a multilevel model with the lmer function, specifying a random intercept for each subject. Save the output summary in an object. 
      



### 2.2 linear vs quadratic

As specified in the FRESHMO paper [Kalisch et al.](https://www.frontiersin.org/articles/10.3389/fpsyg.2021.710493), the relationship between E and P is mostly linear, but could also be quadratic. 
It is best practice to test whether the linear or quadratic relationship is the best fit. 

      Task 2.2 Test wether the E/P relationship is best described with a linear or quadratic model. This is best done with an anova. Interpret the result. You can do so with the simple or multilevel models. 



#### 2.2.1 plot 

To further understand the relationship it can also be useful to plot the E/P lines. Here it would be possible to identify outliers as well (the treatment of outliers is explained in the next advanced part of the script)

        Task 2.2.1 Plot the E/P relationship twice. Once with fitting a linear line and once fitting a quadratic line onto the squatterplot. 




### 2.3 explained variance

As a way of testing the quality of your model you can analyse the explained variance of the model

      Task 2.3 Calculate the R2 of the linear and quadratic model we have just compared. 




### 2.4 decide on model

Things to consider

*Are you interested in an average time period or single timepoints?
*Is your study concluded or not?
*Which model best describes the relationship (linear or quadratic?)
*Are there outliers that could be affecting the fit? (See advanced script)

## 3 calculate SR score 

We will calculate different SR scores heres to showcase how the SR score is calculated. 

### 3.1 single time points

If you want to calculate SR scores for each assessment you need to base it on the multilevel model. Our assessment revealed, that the linear model best describes the relationship. 

#### 3.1.1 extract intercept and slope

We therefore need to extract the intercept and slope from the multilevel model.

      Task 3.1.1 Save the intercept and slope from the multilevel model (2.1.2) in separate objects. 
      


#### 3.1.2 predicted P

We use the extracted intercept and slope to calculate the predicted P based on their E, using a linear equation (y=slope*x+intercept)

The E in the equation needs to be scaled. 

      Task 3.1.2 Calculate the predicted P based on a linear equation, store it as a variable in the dataset
      


#### 3.1.3 SR score

The SR score is then the difference between the actual P and the predicted P, that is the residual of each P to the normative E/P line. 

      Task 3.1.3 Calculate the difference between actual P and predicted P for each participant
      


### 3.2 over entire time period

If we want an SR score over the entire time period than we calculate it on the basis on the model in 2.1.1. Our calculations, however, showed that the quadratic model is the best fit (2.2). 

#### 3.2.1 extract intercept and slope
First, we need to extract the intercept and slopes (in a quadratic model there are two slopes)



#### 3.1.2 predicted P

We then calculate the predicted P base on the quadratic equation. (y=slopeA*x^2+slopeB*x+intercept)

      Task 3.1.2 Calculate the predicted P based on a quadratic equation, store it as a variable in the dataset


#### 3.1.3 SR score

We then again, calculate the difference between actual P and predicted P. 

      Task 3.1.3 Calculate the difference between actual P and predicted P for each participant
      



## 4 Why SR? - SR over time

As an illustration as to why the SR score can offer valuable insights into resilience, we will now examine E, P and SR over time. 

      Task 4: Plot E, P and SR over all 6 time points (separate plots for each variable). Then test the difference between time point 1 and time point 6 to see whether E, P and SR change significantly over time. (you could do this within your plot with ggsignif)



We can see that the change and E and P are significant, but SR is not. Likely the changes in P are explained by changes in E, by controlling for E we can disentangle the causes of these changes.


